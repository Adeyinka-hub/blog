{
  
    
        "post0": {
            "title": "Generative Adversarial Networks: Triplet Training via Pytorch",
            "content": "This is an implementation of the paper:Zieba, Maciej, and Lei Wang. &quot;Training triplet networks with gan.&quot; arXiv preprint arXiv:1704.02227 (2017). . Approach . Data: I have used the MNIST dataset. It has 60,000 training and 10,000 testing samples. The labelled set has N (100 or 200) samples and the unlabelled set had all 60,000 samples. Preprocessing: No preprocessing, as mentioned in the paper. Architecture: I have followed the architecture used in Improved GAN (Salimans et al. 2016). The discriminator outputs M (16 or 32) features. The hyperparameters are same as in TripletGAN. Loss: For pretraining the GAN, I have followed the standard objective (Goodfellow, Ian, et al. 2014) For training the Improved GAN with Triplet loss, I have followed the Improved GAN objective (Salimans et al. 2016) with Triplet Loss in the Discriminator. Training: I initially pretrain the GAN with normal objective, for 50 epochs. Post that I train with the Improved GAN objective for 10 epochs. Postprocessing: No postprocessing, as mentioned in the paper. Classifier: I have used a 9-nearest neighbour classifier on top of M(16 or 32) features extracted by the discriminator. Image Generation: Created an array with noise and sent it to the generator to get the image. Have plotted them in the table below. . Imports . #collapse-hide # Reference # https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier.score # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html # https://github.com/andreasveit/triplet-network-pytorch import math import os import sys from pathlib import Path from pprint import pprint import numpy as np from PIL import Image from scipy.spatial.distance import cdist # from torch import cdist from sklearn import preprocessing from sklearn.metrics import average_precision_score from sklearn.neighbors import KNeighborsClassifier from tqdm.auto import tqdm as tq import torch import torch.nn as nn import torch.optim as optim from livelossplot import PlotLosses from torch.nn import functional as F from torch.nn.parameter import Parameter from torch.utils.data import DataLoader, Dataset, TensorDataset from torchvision import datasets, transforms from torchvision.utils import save_image import matplotlib.pyplot as plt . . References . #collapse-hide # https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py # https://www.kaggle.com/hirotaka0122/triplet-loss-with-pytorch # https://github.com/openai/improved-gan/blob/master/mnist_svhn_cifar10/train_mnist_feature_matching.py # https://github.com/adambielski/siamese-triplet/blob/master/Experiments_MNIST.ipynb # https://github.com/eladhoffer/TripletNet # https://stackoverflow.com/questions/26210471/scikit-learn-gridsearch-giving-valueerror-multiclass-format-is-not-supported . . Utils . #collapse-hide class Arguments(): def __init__(self): self.batch_size=100 self.epochs=10 self.lr=0.003 self.momentum=0.5 self.cuda=torch.cuda.is_available() self.seed=1 self.log_interval=100 self.save_interval=5 self.unlabel_weight=1 self.logdir=&#39;./logfile&#39; self.savedir=&#39;./models&#39; self.load_saved=True args = Arguments() np.random.seed(args.seed) torch.manual_seed(args.seed) results = {} def log_sum_exp(x, axis = 1): m = torch.max(x, dim = 1)[0] return m + torch.log(torch.sum(torch.exp(x - m.unsqueeze(1)), dim = axis)) def reset_normal_param(L, stdv, weight_scale = 1.): assert type(L) == torch.nn.Linear torch.nn.init.normal(L.weight, std=weight_scale / math.sqrt(L.weight.size()[0])) def show_gen_images(gan): num_images=4 arr = gan.draw(num_images) square_dim = num_images//2 f, axarr = plt.subplots(square_dim,square_dim) # f.set_figheight(10) # f.set_figwidth(10) for i in range(square_dim): for j in range(square_dim): axarr[i,j].imshow(arr[i*square_dim+j],cmap=&#39;gray&#39;) axarr[i,j].axis(&#39;off&#39;) # https://github.com/Sleepychord/ImprovedGAN-pytorch/blob/master/functional.py#L13 class LinearWeightNorm(torch.nn.Module): def __init__(self, in_features, out_features, bias=True, weight_scale=None, weight_init_stdv=0.1): super(LinearWeightNorm, self).__init__() self.in_features = in_features self.out_features = out_features self.weight = Parameter(torch.randn(out_features, in_features) * weight_init_stdv) if bias: self.bias = Parameter(torch.zeros(out_features)) else: self.register_parameter(&#39;bias&#39;, None) if weight_scale is not None: assert type(weight_scale) == int self.weight_scale = Parameter(torch.ones(out_features, 1) * weight_scale) else: self.weight_scale = 1 def forward(self, x): W = self.weight * self.weight_scale / torch.sqrt(torch.sum(self.weight ** 2, dim = 1, keepdim = True)) return F.linear(x, W, self.bias) def __repr__(self): return self.__class__.__name__ + &#39;(&#39; + &#39;in_features=&#39; + str(self.in_features) + &#39;, out_features=&#39; + str(self.out_features) + &#39;, weight_scale=&#39; + str(self.weight_scale) + &#39;)&#39; . . Datasets . #collapse-hide # Reference https://github.com/Sleepychord/ImprovedGAN-pytorch/blob/master/Datasets.py def MNISTLabel(class_num): raw_dataset = datasets.MNIST(&#39;../data&#39;, train=True, download=True, transform=transforms.Compose([ transforms.ToTensor(), ])) class_tot = [0] * 10 data = [] labels = [] tot = 0 perm = np.random.permutation(raw_dataset.__len__()) for i in range(raw_dataset.__len__()): datum, label = raw_dataset.__getitem__(perm[i]) if class_tot[label] &lt; class_num: data.append(datum.numpy()) labels.append(label) class_tot[label] += 1 tot += 1 if tot &gt;= 10 * class_num: break times = int(np.ceil(60_000 / len(data))) return TensorDataset(torch.FloatTensor(np.array(data)).repeat(times,1,1,1), torch.LongTensor(np.array(labels)).repeat(times)) def MNISTUnlabel(): raw_dataset = datasets.MNIST(&#39;../data&#39;, train=True, download=True, transform=transforms.Compose([ transforms.ToTensor(), ])) return raw_dataset def MNISTTest(): return datasets.MNIST(&#39;../data&#39;, train=False, download=True, transform=transforms.Compose([ transforms.ToTensor(), ])) # Reference https://github.com/adambielski/siamese-triplet/blob/master/datasets.py#L79 class MNISTTriplet(Dataset): def __init__(self, mnist_dataset): self.mnist_dataset = mnist_dataset.tensors self.train_labels = self.mnist_dataset[1] self.train_data = self.mnist_dataset[0] self.labels_set = set(self.train_labels.numpy()) self.label_to_indices = {} for label in self.labels_set: self.label_to_indices[label] = np.where(self.train_labels.numpy() == label)[0] def __getitem__(self, index): img1, label1 = self.train_data[index], self.train_labels[index].item() positive_index = index while positive_index == index: positive_index = np.random.choice(self.label_to_indices[label1]) negative_label = np.random.choice(list(self.labels_set - set([label1]))) negative_index = np.random.choice(self.label_to_indices[negative_label]) img3 = self.train_data[negative_index] img2 = self.train_data[positive_index] return img1, img2, img3 def __len__(self): return len(self.mnist_dataset[1]) # Reference https://github.com/adambielski/siamese-triplet/blob/master/losses.py class TripletLoss(nn.Module): def __init__(self): super(TripletLoss, self).__init__() def forward(self, anchor, positive, negative, size_average=True): d_positive = torch.sqrt(torch.sum((anchor - positive).pow(2),axis=1)) d_negative = torch.sqrt(torch.sum((anchor - negative).pow(2),axis=1)) z = torch.cat((d_positive.unsqueeze(1),d_negative.unsqueeze(1)),axis=1) z = log_sum_exp(z) return -torch.mean(d_negative) + torch.mean(z) MNISTUnlabel() MNISTTest() . . Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw Processing... Done! . /opt/conda/conda-bld/pytorch_1587428398394/work/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. . Dataset MNIST Number of datapoints: 10000 Root location: ../data Split: Test StandardTransform Transform: Compose( ToTensor() ) . GAN . Architecture: Generator and Discriminator . # https://github.com/Sleepychord/ImprovedGAN-pytorch/blob/master/Nets.py class Discriminator(nn.Module): def __init__(self, input_dim = 28 ** 2, output_dim = 10): super(Discriminator, self).__init__() self.input_dim = input_dim self.output_dim = output_dim self.layers = torch.nn.ModuleList([ LinearWeightNorm(input_dim, 1000), LinearWeightNorm(1000, 500), LinearWeightNorm(500, 250), LinearWeightNorm(250, 250), LinearWeightNorm(250, 250)] ) self.final = LinearWeightNorm(250, output_dim, weight_scale=1) self.reduce = nn.Sequential( nn.Linear(self.output_dim, 1), nn.Sigmoid(), ) def forward(self, x, feature = False, pretrain=False): x = x.view(-1, self.input_dim) noise = torch.randn(x.size()) * 0.3 if self.training else torch.Tensor([0]) if args.cuda: noise = noise.cuda() x = x + noise for i in range(len(self.layers)): m = self.layers[i] x_f = F.relu(m(x)) noise = torch.randn(x_f.size()) * 0.5 if self.training else torch.Tensor([0]) if args.cuda: noise = noise.cuda() x = (x_f + noise) if feature: return x_f out = self.final(x) if pretrain: out = self.reduce(out) return out class Generator(nn.Module): def __init__(self, z_dim, output_dim = 28 * 28): super(Generator, self).__init__() self.z_dim = z_dim self.fc1 = nn.Linear(z_dim, 500, bias = False) self.bn1 = nn.BatchNorm1d(500, affine = False, eps=1e-6, momentum = 0.5) self.fc2 = nn.Linear(500, 500, bias = False) self.bn2 = nn.BatchNorm1d(500, affine = False, eps=1e-6, momentum = 0.5) self.fc3 = LinearWeightNorm(500, output_dim, weight_scale = 1) self.bn1_b = Parameter(torch.zeros(500)) self.bn2_b = Parameter(torch.zeros(500)) nn.init.xavier_uniform_(self.fc1.weight) nn.init.xavier_uniform_(self.fc2.weight) def forward(self, batch_size, draw=None): if draw is None: x = torch.rand(batch_size, self.z_dim) else: x = draw if args.cuda: x = x.cuda() x = F.softplus(self.bn1(self.fc1(x)) + self.bn1_b) x = F.softplus(self.bn2(self.fc2(x)) + self.bn2_b) x = F.softplus(self.fc3(x)) return x . Pretraining . GAN Class . #collapse-hide # Reference: https://github.com/Sleepychord/ImprovedGAN-pytorch/blob/master/ImprovedGAN.py class ImprovedGAN(object): def __init__(self, G, D, labeled, unlabeled, test): self.G = G self.D = D # if(args.mode == &#39;train&#39;): g_name = &#39;G_&#39;+str(D.output_dim)+&#39;_&#39;+str(args.labeled)+&#39;.pkl&#39; d_name = &#39;D_&#39;+str(D.output_dim)+&#39;_&#39;+str(args.labeled)+&#39;.pkl&#39; # else: g_name_pretrain = &#39;pretrain&#39; + &#39;_G_&#39;+str(D.output_dim)+&#39;.pkl&#39; d_name_pretrain = &#39;pretrain&#39; + &#39;_D_&#39;+str(D.output_dim)+&#39;.pkl&#39; if args.mode == &#39;pretrain&#39;: self.g_path = Path(args.savedir) / g_name_pretrain self.d_path = Path(args.savedir) / d_name_pretrain else: self.g_path = Path(args.savedir) / g_name self.d_path = Path(args.savedir) / d_name if os.path.exists(args.savedir) and args.load_saved: print(&#39;Loading model &#39; + args.savedir) if False and os.path.exists(self.g_path): self.G.load_state_dict(torch.load(self.g_path)) self.D.load_state_dict(torch.load(self.d_path)) else: print(&#39;Loaded pretrain&#39;) self.G.load_state_dict(torch.load(Path(args.savedir) / g_name_pretrain)) self.D.load_state_dict(torch.load(Path(args.savedir) / d_name_pretrain)) else: print(&#39;Creating model&#39;) if not os.path.exists(args.savedir): os.makedirs(args.savedir) torch.save(self.G.state_dict(), self.g_path) torch.save(self.D.state_dict(), self.d_path) # self.writer = tensorboardX.SummaryWriter(log_dir=args.logdir) if args.cuda: self.G.cuda() self.D.cuda() self.Doptim = optim.Adam(self.D.parameters(), lr=args.lr, betas= (args.momentum, 0.999)) self.Goptim = optim.Adam(self.G.parameters(), lr=args.lr, betas = (args.momentum,0.999)) self.knn = KNeighborsClassifier(n_neighbors=9) self.tripletloss = TripletLoss() self.drawnoise = torch.rand(4, self.G.z_dim) self.labeled = labeled self.unlabeled = unlabeled self.test = test def get_features(self,dataset): loader = DataLoader(dataset, batch_size = args.batch_size, shuffle=True, drop_last=True, num_workers = 4) X = [] y = [] for (data,label) in loader: data = data.cuda() X += self.D(data) y += label # del data del loader,data X = torch.stack(X).data.cpu().numpy() y = torch.LongTensor(y).data.cpu().numpy() # X = torch.stack(X) # y = torch.LongTensor(y) return X,y def trainknn(self): X,y = self.get_features(self.unlabeled) self.knn.fit(X,y) print(&quot;Fit done&quot;) del X,y def calc_mAP(self,test_features, testy, train_features, trainy): Y = cdist(test_features,train_features) ind = np.argsort(Y,axis=1) print(&quot;Done argsort&quot;) del Y,train_features prec = 0.0 num_classes = 10 acc = [0.0] * num_classes test_len = len(test_features) # print(&quot;testlen&quot;,test_len) for k in range(test_len): class_values = trainy[ind[k,:]] y_true = (testy[k] == class_values) # print(&quot;ylen&quot;,y_true.shape[0]) y_scores = np.arange(y_true.shape[0],0,-1) prec += average_precision_score(y_true, y_scores) for n in range(num_classes): a = class_values[0:(n+1)] counts = np.bincount(a) b = np.where(counts==np.max(counts))[0] if testy[k] in b: acc[n] = acc[n] + (1.0/float(len(b))) prec = prec/float(test_len) acc= [x / float(test_len) for x in acc] del ind,class_values,y_true,y_scores return np.mean(acc)*100,prec def evalknn(self,results): test_features, testy = self.get_features(self.test) train_features, trainy = self.get_features(self.unlabeled) accuracy,mAP = self.calc_mAP(test_features, testy, train_features, trainy) del test_features,testy,train_features,trainy results[args.mode+&#39;_&#39;+str(args.features)+&#39;_&#39;+str(args.labeled)] = [accuracy,mAP] return accuracy,mAP def draw(self, batch_size): self.G.eval() return self.G(batch_size,draw=self.drawnoise).view((batch_size,28,28)).data.cpu().numpy() . . . Pretrain GAN . Original GAN objective . def pretrain(self): # Reference: https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py plotlosses = PlotLosses(groups={&#39;loss&#39;: [&#39;generator&#39;, &#39;discriminator&#39;]}) # Tensor = torch.cuda.FloatTensor if args.cuda else torch.FloatTensor bce_loss = torch.nn.BCELoss() if args.cuda: bce_loss.cuda() dataloader = DataLoader(self.unlabeled, batch_size = args.batch_size, shuffle=True, drop_last=True, num_workers = 4) for epoch in tq(range(args.epochs)): losses = {&#39;discriminator&#39;:0,&#39;generator&#39;:0} for i, (imgs, _) in enumerate(dataloader): valid = torch.ones((imgs.size(0), 1)) fake = torch.zeros((imgs.size(0), 1)) train_imgs = imgs generated_images = self.G(args.batch_size) if args.cuda: valid, fake, train_imgs, generated_images = valid.cuda(), fake.cuda(), train_imgs.cuda(), generated_images.cuda() generator_loss = bce_loss(self.D(generated_images,pretrain=True), valid) self.Goptim.zero_grad() generator_loss.backward() self.Goptim.step() real_loss = bce_loss(self.D(train_imgs,pretrain=True), valid) fake_loss = bce_loss(self.D(generated_images.detach(),pretrain=True), fake) discriminator_loss = (fake_loss + real_loss) / 2 self.Doptim.zero_grad() discriminator_loss.backward() self.Doptim.step() losses[&#39;generator&#39;] += generator_loss.item() losses[&#39;discriminator&#39;] += discriminator_loss.item() num_batches = len(self.unlabeled) / args.batch_size for key in losses: losses[key] /= num_batches plotlosses.update(losses) plotlosses.send() if (epoch + 1) % args.save_interval == 0: torch.save(self.G.state_dict(), self.g_path) torch.save(self.D.state_dict(), self.d_path) ImprovedGAN.pretrain = pretrainargs.load_saved=False args.epochs=50 args.mode = &#39;pretrain&#39; # m=16 n=100 args.labeled=100 args.features=16 gan = ImprovedGAN(Generator(z_dim=100), Discriminator(output_dim=args.features), MNISTLabel(args.labeled/10), MNISTUnlabel(), MNISTTest()) gan.pretrain() # gan.trainknn() print(gan.evalknn(results)) show_gen_images(gan) . m=16 . args.load_saved=True args.epochs=50 args.mode = &#39;pretrain&#39; # m=16 n=100 args.labeled=100 args.features=16 gan = ImprovedGAN(Generator(z_dim=100), Discriminator(output_dim=args.features), MNISTLabel(args.labeled/10), MNISTUnlabel(), MNISTTest()) gan.pretrain() # gan.trainknn() print(gan.evalknn(results)) show_gen_images(gan) . Loss generator (min: 1.549, max: 2.075, cur: 1.775) discriminator (min: 0.391, max: 0.429, cur: 0.409) Done w cdist (30.023548015873082, 0.15303857492111891) . m=32 . # m=32 n=100 args.features=32 gan = ImprovedGAN(Generator(z_dim=100), Discriminator(output_dim=args.features), MNISTLabel(args.labeled/10), MNISTUnlabel(), MNISTTest()) gan.pretrain() # gan.trainknn() print(gan.evalknn(results)) show_gen_images(gan) . Loss generator (min: 1.724, max: 2.207, cur: 1.793) discriminator (min: 0.373, max: 0.414, cur: 0.406) Done w cdist (44.4227202380953, 0.1734273342974863) . Main Training . Improved GAN Objective + Triplet loss . # Reference: https://github.com/Sleepychord/ImprovedGAN-pytorch/blob/master/ImprovedGAN.py def train_Discriminator(self, x1, x2, x3, x_unlabel): output_unlabel, output_fake = self.D(x_unlabel), self.D(self.G(x_unlabel.size()[0]).view(x_unlabel.size()).detach()) loss_supervised = self.tripletloss(self.D(x1), self.D(x2), self.D(x3)) logz_unlabel, logz_fake = log_sum_exp(output_unlabel), log_sum_exp(output_fake) loss_unsupervised = 0.5 * torch.mean(F.softplus(logz_fake)) + 0.5 * -torch.mean(logz_unlabel) + 0.5 * torch.mean(F.softplus(logz_unlabel)) loss = args.unlabel_weight * loss_unsupervised + loss_supervised self.Doptim.zero_grad() loss.backward() self.Doptim.step() return loss_supervised.item(), loss_unsupervised.item() def train_Generator(self, x_unlabel): fake = self.G(args.batch_size).view(x_unlabel.size()) mom_gen = self.D(fake, feature=True).mean(dim=0) mom_unlabel = self.D(x_unlabel, feature=True).mean(dim=0) loss_feature_matching = torch.mean((mom_gen - mom_unlabel).pow(2)) self.Goptim.zero_grad() self.Doptim.zero_grad() loss_feature_matching.backward() self.Goptim.step() return loss_feature_matching.item() def train(self): plotlosses = PlotLosses(groups={&#39;loss&#39;: [&#39;supervised&#39;, &#39;unsupervised&#39;,&#39;generator&#39;]}) for epoch in tq(range(args.epochs)): self.G.train() self.D.train() unlabel_loader1 = DataLoader(self.unlabeled, batch_size = args.batch_size, shuffle=True, drop_last=True, num_workers = 4) unlabel_loader2 = DataLoader(self.unlabeled, batch_size = args.batch_size, shuffle=True, drop_last=True, num_workers = 4).__iter__() label_loader = DataLoader(self.labeled, batch_size = args.batch_size, shuffle=True, drop_last=True, num_workers = 4).__iter__() # loss_supervised = loss_unsupervised = loss_generator = 0. losses = {&#39;supervised&#39;:0,&#39;unsupervised&#39;:0,&#39;generator&#39;:0} for (unlabel1, _) in unlabel_loader1: unlabel2, _ = unlabel_loader2.next() x1,x2,x3 = label_loader.next() if args.cuda: x1, x2, x3, unlabel1, unlabel2 = x1.cuda(), x2.cuda(), x3.cuda(), unlabel1.cuda(), unlabel2.cuda() l_supervised, l_unsupervised = self.train_Discriminator(x1, x2, x3, unlabel1) losses[&#39;unsupervised&#39;] += l_unsupervised losses[&#39;supervised&#39;] += l_supervised generator_loss = self.train_Generator(unlabel2) if epoch &gt; 1 and generator_loss &gt; 1: generator_loss = self.train_Generator(unlabel2) losses[&#39;generator&#39;] += generator_loss batch_num = len(self.unlabeled) // args.batch_size for key in losses: losses[key] /= batch_num plotlosses.update(losses) plotlosses.send() if (epoch + 1) % args.save_interval == 0: torch.save(self.D.state_dict(), self.d_path) torch.save(self.G.state_dict(), self.g_path) ImprovedGAN.train = train ImprovedGAN.train_Discriminator = train_Discriminator ImprovedGAN.train_Generator = train_Generator . Triplet M=16 N=100 . %%time args.load_saved=True args.epochs=70 args.labeled=100 args.features=16 args.mode = &#39;train&#39; gan = ImprovedGAN(Generator(z_dim=100), Discriminator(output_dim=args.features), MNISTTriplet(MNISTLabel(args.labeled/10)), MNISTUnlabel(), MNISTTest()) gan.train() # gan.trainknn() print(gan.evalknn(results)) show_gen_images(gan) . Loss supervised (min: 0.000, max: 0.079, cur: 0.000) unsupervised (min: 0.351, max: 0.453, cur: 0.422) generator (min: 0.511, max: 1.353, cur: 0.511) Done argsort (96.17309999999998, 0.9155653700006265) CPU times: user 53min, sys: 2min 10s, total: 55min 11s Wall time: 1h 5min 43s . Triplet M=16 N=200 . args.load_saved=True args.epochs=70 args.labeled=200 args.features=16 gan = ImprovedGAN(Generator(z_dim=100), Discriminator(output_dim=args.features), MNISTTriplet(MNISTLabel(args.labeled/10)), MNISTUnlabel(), MNISTTest()) gan.train() # gan.trainknn() print(gan.evalknn(results)) show_gen_images(gan) . Loss supervised (min: 0.000, max: 0.106, cur: 0.000) unsupervised (min: 0.368, max: 0.463, cur: 0.413) generator (min: 0.487, max: 1.185, cur: 0.487) Done argsort (96.44481666666665, 0.931573062438397) . Triplet M=32 N=100 . args.load_saved=True args.epochs=70 args.labeled=100 args.features=32 gan = ImprovedGAN(Generator(z_dim=100), Discriminator(output_dim=args.features), MNISTTriplet(MNISTLabel(args.labeled/10)), MNISTUnlabel(), MNISTTest()) gan.train() # gan.trainknn() print(gan.evalknn(results)) show_gen_images(gan) . Loss supervised (min: 0.000, max: 0.098, cur: 0.000) unsupervised (min: 0.355, max: 0.485, cur: 0.430) generator (min: 0.680, max: 2.149, cur: 0.699) Done argsort (95.97931666666668, 0.9031129058147167) . Results . # batchwise iterate through data # fit the batch for the classifier # get accuracy and mAP pprint(results) . {&#39;train_16_100&#39;: [96.17309999999998, 0.9155653700006265], &#39;train_16_200&#39;: [96.44481666666665, 0.931573062438397], &#39;train_32_100&#39;: [95.97931666666668, 0.9031129058147167]} . Clear GPU . #collapse-hide # import gc # def dump_tensors(gpu_only=True): # torch.cuda.empty_cache() # total_size = 0 # for obj in gc.get_objects(): # # print(obj) # try: # if torch.is_tensor(obj): # if obj.is_cuda: # del obj # gc.collect() # elif hasattr(obj, &quot;data&quot;) and torch.is_tensor(obj.data): # if not gpu_only or obj.is_cuda: # del obj # gc.collect() # except Exception as e: # pass # dump_tensors() . .",
            "url": "https://rohanrajpal.github.io/blog/deep%20learning/2020/06/30/GAN-Triplet.html",
            "relUrl": "/deep%20learning/2020/06/30/GAN-Triplet.html",
            "date": " • Jun 30, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Interpreting the C++ programming language",
            "content": "Usage . To run the selected code cell, hit Shift + Enter . Output and error streams . std::cout and std::cerr are redirected to the notebook frontend. . #include &lt;iostream&gt; std::cout &lt;&lt; &quot;some output&quot; &lt;&lt; std::endl; . some output . std::cerr &lt;&lt; &quot;some error&quot; &lt;&lt; std::endl; . some error . #include &lt;stdexcept&gt; . // throw std::runtime_error(&quot;Unknown exception&quot;); . Omitting the ; in the last statement of a cell results in an output being printed . int j = 5; . j . 5 . Interpreting the C++ programming language . cling has a broad support of the features of C++. You can define functions, classes, templates, etc ... . Functions . double sqr(double a) { return a * a; } . double a = 2.5; double asqr = sqr(a); asqr . 6.2500000 . Classes . class Foo { public: virtual ~Foo() {} virtual void print(double value) const { std::cout &lt;&lt; &quot;Foo value = &quot; &lt;&lt; value &lt;&lt; std::endl; } }; . Foo bar; bar.print(1.2); . Foo value = 1.2 . Polymorphism . class Bar : public Foo { public: virtual ~Bar() {} virtual void print(double value) const { std::cout &lt;&lt; &quot;Bar value = &quot; &lt;&lt; 2 * value &lt;&lt; std::endl; } }; . Foo* bar2 = new Bar; bar2-&gt;print(1.2); delete bar2; . Bar value = 2.4 . Templates . #include &lt;typeinfo&gt; template &lt;class T&gt; class FooT { public: explicit FooT(const T&amp; t) : m_t(t) {} void print() const { std::cout &lt;&lt; typeid(T).name() &lt;&lt; &quot; m_t = &quot; &lt;&lt; m_t &lt;&lt; std::endl; } private: T m_t; }; template &lt;&gt; class FooT&lt;int&gt; { public: explicit FooT(const int&amp; t) : m_t(t) {} void print() const { std::cout &lt;&lt; &quot;m_t = &quot; &lt;&lt; m_t &lt;&lt; std::endl; } private: int m_t; }; . FooT&lt;double&gt; foot1(1.2); foot1.print(); . d m_t = 1.2 . FooT&lt;int&gt; foot2(4); foot2.print(); . m_t = 4 . C++11 / C++14 support . class Foo11 { public: Foo11() { std::cout &lt;&lt; &quot;Foo11 default constructor&quot; &lt;&lt; std::endl; } Foo11(const Foo11&amp;) { std::cout &lt;&lt; &quot;Foo11 copy constructor&quot; &lt;&lt; std::endl; } Foo11(Foo11&amp;&amp;) { std::cout &lt;&lt; &quot;Foo11 move constructor&quot; &lt;&lt; std::endl; } }; . Foo11 f1; Foo11 f2(f1); Foo11 f3(std::move(f1)); . Foo11 default constructor Foo11 copy constructor Foo11 move constructor . #include &lt;vector&gt; std::vector&lt;int&gt; v = { 1, 2, 3}; auto iter = ++v.begin(); v . { 1, 2, 3 } . *iter . 2 . ... and also lambda, universal references, decltype, etc ... . Documentation and completion . Documentation for types of the standard library is retrieved on cppreference.com. | The quick-help feature can also be enabled for user-defined types and third-party libraries. More documentation on this feature is available at https://xeus-cling.readthedocs.io/en/latest/inline_help.html. | . ?std::vector . Using the display_data mechanism . For a user-defined type T, the rich rendering in the notebook and JupyterLab can be enabled by by implementing the function nl::json mime_bundle_repr(const T&amp; im), which returns the JSON mime bundle for that type. . More documentation on the rich display system of Jupyter and Xeus-cling is available at https://xeus-cling.readthedocs.io/en/latest/rich_display.html . Image example . #include &lt;string&gt; #include &lt;fstream&gt; #include &quot;nlohmann/json.hpp&quot; #include &quot;xtl/xbase64.hpp&quot; namespace nl = nlohmann; namespace im { struct image { inline image(const std::string&amp; filename) { std::ifstream fin(filename, std::ios::binary); m_buffer &lt;&lt; fin.rdbuf(); } std::stringstream m_buffer; }; nl::json mime_bundle_repr(const image&amp; i) { auto bundle = nl::json::object(); bundle[&quot;image/png&quot;] = xtl::base64encode(i.m_buffer.str()); return bundle; } } . im::image marie(&quot;images/marie.png&quot;); marie . Audio example . #include &lt;string&gt; #include &lt;fstream&gt; #include &quot;nlohmann/json.hpp&quot; #include &quot;xtl/xbase64.hpp&quot; namespace nl = nlohmann; namespace au { struct audio { inline audio(const std::string&amp; filename) { std::ifstream fin(filename, std::ios::binary); m_buffer &lt;&lt; fin.rdbuf(); } std::stringstream m_buffer; }; nl::json mime_bundle_repr(const audio&amp; a) { auto bundle = nl::json::object(); bundle[&quot;text/html&quot;] = std::string(&quot;&lt;audio controls= &quot;controls &quot;&gt;&lt;source src= &quot;data:audio/wav;base64,&quot;) + xtl::base64encode(a.m_buffer.str()) + &quot; &quot; type= &quot;audio/wav &quot; /&gt;&lt;/audio&gt;&quot;; return bundle; } } . au::audio drums(&quot;audio/audio.wav&quot;); drums . Display . #include &quot;xcpp/xdisplay.hpp&quot; . xcpp::display(drums); . Update-display . #include &lt;string&gt; #include &quot;xcpp/xdisplay.hpp&quot; #include &quot;nlohmann/json.hpp&quot; namespace nl = nlohmann; namespace ht { struct html { inline html(const std::string&amp; content) { m_content = content; } std::string m_content; }; nl::json mime_bundle_repr(const html&amp; a) { auto bundle = nl::json::object(); bundle[&quot;text/html&quot;] = a.m_content; return bundle; } } // A blue rectangle ht::html rect(R&quot;( &lt;div style=&#39; width: 90px; height: 50px; line-height: 50px; background-color: blue; color: white; text-align: center;&#39;&gt; Original &lt;/div&gt;)&quot;); . xcpp::display(rect, &quot;some_display_id&quot;); . Updated // Update the rectangle to be red rect.m_content = R&quot;( &lt;div style=&#39; width: 90px; height: 50px; line-height: 50px; background-color: red; color: white; text-align: center;&#39;&gt; Updated &lt;/div&gt;)&quot;; xcpp::display(rect, &quot;some_display_id&quot;, true); . Clear output . #include &lt;chrono&gt; #include &lt;iostream&gt; #include &lt;thread&gt; #include &quot;xcpp/xdisplay.hpp&quot; . std::cout &lt;&lt; &quot;hello&quot; &lt;&lt; std::endl; std::this_thread::sleep_for(std::chrono::seconds(1)); xcpp::clear_output(); // will flicker when replacing &quot;hello&quot; with &quot;goodbye&quot; std::this_thread::sleep_for(std::chrono::seconds(1)); std::cout &lt;&lt; &quot;goodbye&quot; &lt;&lt; std::endl; . goodbye . std::cout &lt;&lt; &quot;hello&quot; &lt;&lt; std::endl; std::this_thread::sleep_for(std::chrono::seconds(1)); xcpp::clear_output(true); // prevents flickering std::this_thread::sleep_for(std::chrono::seconds(1)); std::cout &lt;&lt; &quot;goodbye&quot; &lt;&lt; std::endl; . goodbye . Magics . Magics are special commands for the kernel that are not part of the C++ language. . They are defined with the symbol % for a line magic and %% for a cell magic. . More documentation for magics is available at https://xeus-cling.readthedocs.io/en/latest/magics.html. . #include &lt;algorithm&gt; #include &lt;vector&gt; . std::vector&lt;double&gt; to_shuffle = {1, 2, 3, 4}; . %timeit std::random_shuffle(to_shuffle.begin(), to_shuffle.end()); . 317 ns +- 9.09 ns per loop (mean +- std. dev. of 7 runs 1000000 loops each) . . GitHub repository: https://github.com/QuantStack/xtensor/ | Online documentation: https://xtensor.readthedocs.io/ | NumPy to xtensor cheat sheet: http://xtensor.readthedocs.io/en/latest/numpy.html | . xtensor is a C++ library for manipulating N-D arrays with an API very similar to that of numpy. . #include &lt;iostream&gt; #include &quot;xtensor/xarray.hpp&quot; #include &quot;xtensor/xio.hpp&quot; #include &quot;xtensor/xview.hpp&quot; xt::xarray&lt;double&gt; arr1 {{1.0, 2.0, 3.0}, {2.0, 5.0, 7.0}, {2.0, 5.0, 7.0}}; xt::xarray&lt;double&gt; arr2 {5.0, 6.0, 7.0}; xt::view(arr1, 1) + arr2 . 7. . | . 11. . | . 14. . | . Together with the C++ Jupyter kernel, xtensor offers a similar experience as NumPy in the Python Jupyter kernel, including broadcasting and universal functions. . #include &lt;iostream&gt; #include &quot;xtensor/xarray.hpp&quot; #include &quot;xtensor/xio.hpp&quot; . xt::xarray&lt;int&gt; arr {1, 2, 3, 4, 5, 6, 7, 8, 9}; arr.reshape({3, 3}); std::cout &lt;&lt; arr; . {{1, 2, 3}, {4, 5, 6}, {7, 8, 9}} . #include &quot;xtensor-blas/xlinalg.hpp&quot; . xt::xtensor&lt;double, 2&gt; m = {{1.5, 0.5}, {0.7, 1.0}}; std::cout &lt;&lt; &quot;Matrix rank: &quot; &lt;&lt; std::endl &lt;&lt; xt::linalg::matrix_rank(m) &lt;&lt; std::endl; std::cout &lt;&lt; &quot;Matrix inverse: &quot; &lt;&lt; std::endl &lt;&lt; xt::linalg::inv(m) &lt;&lt; std::endl; std::cout &lt;&lt; &quot;Eigen values: &quot; &lt;&lt; std::endl &lt;&lt; xt::linalg::eigvals(m) &lt;&lt; std::endl; . Matrix rank: 2 Matrix inverse: {{ 0.869565, -0.434783}, {-0.608696, 1.304348}} Eigen values: { 1.892262+0.i, 0.607738+0.i} . xt::xarray&lt;double&gt; arg1 = xt::arange&lt;double&gt;(9); xt::xarray&lt;double&gt; arg2 = xt::arange&lt;double&gt;(18); arg1.reshape({3, 3}); arg2.reshape({2, 3, 3}); std::cout &lt;&lt; xt::linalg::dot(arg1, arg2) &lt;&lt; std::endl; . {{{ 15., 18., 21.}, { 42., 45., 48.}}, {{ 42., 54., 66.}, { 150., 162., 174.}}, {{ 69., 90., 111.}, { 258., 279., 300.}}} .",
            "url": "https://rohanrajpal.github.io/blog/2020/06/20/xcpp.html",
            "relUrl": "/2020/06/20/xcpp.html",
            "date": " • Jun 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Competitive Programming: C++ Cheat Sheet",
            "content": "Syntax and tricks for C++. . Improve speed . Leetcode . static int fastio = []() { std::ios::sync_with_stdio(false); std::cin.tie(NULL); std::cout.tie(0); return 0; }(); . Normal compile . #define fast ios_base::sync_with_stdio(false),cin.tie(NULL) . Header Template . #include&lt;bits/stdc++.h&gt; #define ll long long #define ld long double #define vll vector&lt;ll&gt; #define vii vector&lt;int&gt; #define vvll vector&lt; vll &gt; #define pll pair&lt;ll ,ll &gt; #define MOD 1000000007 #define rall(v) v.rbegin(),v.rend() #define fst first #define mp make_pair #define pb push_back #define fast ios_base::sync_with_stdio(false),cin.tie(NULL) #define int long long #define endl &quot; n&quot; #define all(v) v.begin(),v.end() #define scd second #define for1(i,n) for(ll (i) = 1 ; (i) &lt;= (n) ; ++(i)) #define forr(i,n) for(ll (i) = (n)-1 ; (i)&gt;=0 ; --(i)) #define forn(i,n) for(ll (i) = 0 ; (i) &lt; (n) ; ++(i)) #define forab(i,a,b,c) for(ll (i) = a ; (i) &lt;= (b) ; (i)+=(c)) #define mst(A) memset( (A) , 0 , sizeof(A) ); #define tc() int t; cin &gt;&gt; t ; while (t--) using namespace std; . Comparators . class Solution { public: int twoCitySchedCost(vector&lt;vector&lt;int&gt;&gt;&amp; costs) { sort(costs.begin(),costs.end(),[](auto &amp;i1,auto &amp;i2) -&gt; bool {return i1[0] - i1[1] &lt; i2[0] - i2[1];}); int sum=0; for(int i=0;i&lt;costs.size()/2;i++){ sum += costs[i][0] + costs[costs.size()-1-i][1]; } return sum; } }; .",
            "url": "https://rohanrajpal.github.io/blog/competitive%20programming/2020/06/20/cpp-cheat-sheet.html",
            "relUrl": "/competitive%20programming/2020/06/20/cpp-cheat-sheet.html",
            "date": " • Jun 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Competitive Programming: Problems and Tricks on various concepts.",
            "content": "In this blog I’ll document all the challenging competitive programming problems with the focus on motivation rather than the solution. . 5 Problem Solving Tips for Cracking Coding Interview Questions [Youtube] . Tip #1: Come up with a brute-force solution - Tip #2: Think of a simpler version of the problem - Tip #3: Think with simpler examples -&gt; try noticing a pattern - Tip #4: Use some visualization - Tip #5: Test your solution on a few examples - . Study Plan . Leetcode top interview questions Generic preparation tips I give to freshers to clear any algorithmic interview anywhere: 1) Data Structures and Algorithms (online course on NPTEL by Naveen Garg) 2) Analysis and Design of Algorithms (online course on Coursera by Tim Roughgarden, part 1 and part 2 3) 1 programming language, everything about it including inbuilt things like maps, binary search, sort, stacks, priority queues, vectors / lists, pairs etc 4) all Codeforces contests from now till interviews 5) 200+ Leetcode questions . Take competitive programming course if you haven’t done that already. . Best of the best blogs . Yash girdhar Good plan of how to prepare | . | Abinav Bhardwaj Screenshots of contests of various company contests | . | A sheet of leetcode probs | Resources by google | Five Essential Phone Screen Questions by Steve Yegge | How to: Work at Google — Candidate Coaching Session for Technical Interviewing | How To Get Hired – What CS Students Need to Know | Topcoder tutorials | Google resource | . Array . Remove Duplicates from Sorted Array . Leetcode . The question is interesting because you have to ensure the array is modified, too, not just return the answer. . class Solution { public: int removeDuplicates(vector&lt;int&gt;&amp; nums) { int j=!nums.empty(); for(int i=1;i&lt;nums.size();i++){ if(nums[i]&gt;nums[j-1]){ nums[j++]=nums[i]; } } return j; } }; . Dynamic Programming . A smart brute force and your algorithm goes from cubic to linear complexity. That’s dynamic programming. . Coin Change 2 . Leetcode . We can use a coin infinite number of times, but how many times do we need? We know that the amounts a coin can make, so why not just store the count of the amount that we have made? Out of all the possible amount, if you add that value to . Say we have used a part of the array to build our answer, now we come across a new coin: For a coin of value c, if previously we had n number of ways to make vaulue i-c, in how many ways can we make i? ways[i-c] + already existing ways to make i. . class Solution { public: int change(int amount, vector&lt;int&gt;&amp; coins) { vector&lt;int&gt; dp(amount+1); dp[0] =1; for(auto&amp; coin: coins){ for(int i=coin;i&lt;=amount;i++) dp[i] += dp[i-coin]; } return dp[amount]; } }; . Dungeon Game . Leetcode Mistakes I made: please dry run before coding . Notice that the minimum initial health will be at least 1 even if we have an example like [1,0,0]. . When we reach the queen, say value in that cell is -3, so we must have 1-(-2) health to save the queen. The minimum initial length at the bottom right should be max(1,1-dungeon[i][j]). . Now at any i,j, if we need 3 points of health for future, we will add requirements of health for current i,j, so health will be 3 - (-4). We should have max(1,min(dp[i+1][j],dp[i][j+1])-dungeon[i][j]) health. . Top down DP (Easier to understand) . class Solution { public: int m,n; int calculateMinimumHP(vector&lt;vector&lt;int&gt;&gt;&amp; dungeon) { m = dungeon.size();n=dungeon[0].size(); vector&lt;vector&lt;int&gt;&gt; dp(m,vector&lt;int&gt;(n,0)); return calcMin(dungeon,dp,0,0); } int calcMin(vector&lt;vector&lt;int&gt;&gt;&amp; dungeon, vector&lt;vector&lt;int&gt;&gt;&amp; dp, int i, int j){ if(i&gt;=m or j&gt;=n) return INT_MAX; if(i==m-1 and j==n-1) return max(1,1 - dungeon[i][j]); if(dp[i][j]) return dp[i][j]; return dp[i][j] = max(1,min(calcMin(dungeon,dp,i,j+1),calcMin(dungeon,dp,i+1,j)) - dungeon[i][j]); } }; . Bottom up DP (Space optimized) We dont really need to make an extra array when doing bottom up. . class Solution { public: int calculateMinimumHP(vector&lt;vector&lt;int&gt;&gt;&amp; dungeon) { int m=dungeon.size(),n=dungeon[0].size(); for(int i=m-1;i&gt;=0;i--) for(int j=n-1;j&gt;=0;j--) if(i==m-1 and j==n-1) dungeon[i][j] = max(1,1-dungeon[i][j]); else if(i==m-1) dungeon[i][j] = max(1,dungeon[i][j+1]-dungeon[i][j]); else if(j==n-1) dungeon[i][j] = max(1,dungeon[i+1][j]-dungeon[i][j]); else dungeon[i][j] = max(1,min(dungeon[i][j+1],dungeon[i+1][j])-dungeon[i][j]); return dungeon[0][0]; } }; . Maths . Permutation Sequence . Leetcode | Solution Reference . C++ has an inbuilt function next_permuation . class Solution { public: string getPermutation(int n, int k) { string seq=&quot;&quot;; for(int i=1;i&lt;=n;i++) seq+=(&#39;1&#39;+i-1); while(--k&gt;0) next_permutation(seq.begin(),seq.end()); return seq; } }; . Complexity: O(k) . Can we do better? Absolutely! Let’s take n=4; the permutations will be . 1 - (2,3,4) (2,4,3) ... 2 - (1,4,3) ... 3 - (1,2,4) ... 4 - (1,2,3) ... . On removing the first index, the rest of the permutations are repeating. If we want to find the 22nd permutation. The first index will be k/(n-1)! = 21 / (4-1)! = 21 / 3! = 3. 3rd index of [1,2,3,4] is 4. So the first number would be 4. The permutation till here is “4” Now how do we find the index in the remaining combinations? Before the index, we have covered index*(n-1)! permutations. Subtracting this from k will give us the index for the new sub problem k = k % (n-1) = 21 % (4-1)! = 21 - 18 = 3 We also remove 4 from the set now. Set is now . 1 - (2,3) (3,2) 2 - (3,1) ... 3 - (1,2) (2,1) . index = 3 / (n-2)! = 3 / 2! = 1 Now the index at 1 in [1, 2, 3] is 2. Permutation now is “42” Left out set is [1 , 3] k = 3 % 2! = 1 index = 1 / 1! = 1. Elem at 1 is 3, permutation is “423” Finally, k = 1 - 1*0! = 0 Adding the left-out element perm is finally = “4231” . class Solution { public: string getPermutation(int n, int k) { string seq=&quot;&quot;;k--; vector&lt;int&gt; nums,fact(n+1,1); for(int i=1;i&lt;=n;i++) nums.push_back(i); for(int i=1;i&lt;=n;i++) fact[i] = fact[i-1]*i; for(;n&gt;0;n--){ int idx = k / fact[n-1]; seq += (nums[idx]-1) + &#39;1&#39;; k %= fact[n-1]; nums.erase(nums.begin()+idx); } return seq; } }; . Single Number (XOR) . Leetcode link . If we XOR a number with itself, it nulls out. So XOR all the numbers in the array and voila! . class Solution { public: int singleNumber(vector&lt;int&gt;&amp; nums) { int ans=0; for(int &amp;i:nums){ ans ^= i; } return ans; } }; . Single Number II (Bit Manipulation) . Leetcode | Solution reference . Of course we can use a dictionary, but can we do it in constant memory? We can, but it’s not that straightforward. . We’ll have to think of integers in terms of bits to solve this problem. . Aim: Develop a counter which . Resets after k elements | Counter should be unaffected by 0 | It should increment by 1 if it sees one | . class Solution { public: int singleNumber(vector&lt;int&gt;&amp; nums) { int x1=0,x2=0,mask; for(int i=0;i&lt;nums.size();i++){ x2 ^= x1 &amp; nums[i]; x1 ^= nums[i]; mask = ~(x1 &amp; x2); x1 &amp;= mask; x2 &amp;= mask; } return x1; } }; . Graphs . Cheapest Flights Within K Stops (Dijkstra) . We can take the Dijkstra algorithm and modify it so that distances greater than k hops will not be considered. . #define INF 0x3f3f3f3f class Point { public: int node; int dist; int hops; Point(int a, int b, int c){ node=a;dist=b;hops=c; } }; class myComparator { public: int operator() (const Point&amp; p1, const Point&amp; p2) { return p1.dist &gt; p2.dist; } }; class Solution { public: int findCheapestPrice(int n, vector&lt;vector&lt;int&gt;&gt;&amp; flights, int src, int dst, int K) { if(flights.size()==0 or n==0) return -1; vector&lt;vector&lt;pair&lt;int,int&gt;&gt;&gt; adj(n,vector&lt;pair&lt;int,int&gt;&gt;(0)); for(int i=0;i&lt;flights.size();i++){ adj[flights[i][0]].push_back(make_pair(flights[i][1],flights[i][2])); } priority_queue &lt;Point, vector&lt;Point&gt;, myComparator&gt; pq; pq.push(Point(src,0,0)); vector&lt;int&gt; dist(n,INF); dist[src]=0; while(!pq.empty()){ Point cur = pq.top(); pq.pop(); if(cur.node == dst) return cur.dist; if(cur.hops &gt; K) continue; for(int i=0;i&lt;adj[cur.node].size();i++) if(dist[adj[cur.node][i].first] &gt; cur.dist+adj[cur.node][i].second) pq.push(Point(adj[cur.node][i].first,cur.dist+adj[cur.node][i].second,cur.hops+1)); } return -1; } }; . Tree . Count Complete Tree Nodes . Leetcode link . class Solution { public: int countNodes(TreeNode* root) { int height = findHeight(root); return height&lt; 0? 0: findHeight(root-&gt;right)==height-1? (1&lt;&lt;height) + countNodes(root-&gt;right):(1&lt;&lt;(height-1)) + countNodes(root-&gt;left); } int findHeight(TreeNode* node){ return node==NULL?-1 : 1+findHeight(node-&gt;left); } }; . Binary Search . Longest Duplicate Substring (Rolling Hashes) . Leetcode link . Do a binary search on length Have the least length as 0 and highest as n-1 | Take the mid-length, if you find a duplicate, try for a bigger length | If not, try for a smaller length | . | Sliding window (Robin Karp) Have a look at the rolling hash approach | . | robin karp solution | c++ 17 solution | .",
            "url": "https://rohanrajpal.github.io/blog/competitive%20programming/2020/06/18/competitive-programming.html",
            "relUrl": "/competitive%20programming/2020/06/18/competitive-programming.html",
            "date": " • Jun 18, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "The Virtual Internship: My Experience [Draft]",
            "content": "These summers, I interned at a one of the largest travel companies of the world, Expedia. Who knew during a pandemic when travel is at an all time low, Expedia had a great intern program planned for us! . Luckily, my previous intern was remote too, so I was kind of used to work from home. In this internship, I majorly worked on cloud and data engineering. . . The induction week . Expedia organized an induction week for us, and it involved workshops on various topics: how to become a better leader, designer, and we even got to know how Expedia operates. We kept on having workshops throughout our intern, and there was a lot to learn, so much that I wrote a separate blog about it :grin:. . My setup . Not the fanciest setup out there, but enough to get the work done :) My Team - Vrbo: Stayx dot net modernization pod . I was a part of the stay experience team, which looks after the post-booking experience for a traveler. The team was divided into pods, and I worked on the .Net Stack Modernisation pod. I worked on ### making a microservice and how to deploy it to the cloud. . The project . Aim: Create a spark job to sync data from MongoDB to S3 . A quick revision . This blog is going to get more technical now, just keeping this for a quick reference if you have a doubt. . What’s MongoDB? It’s a NoSQL database. What’s scala? It’s a programming language widely used for data management. I did all my coding in the Scala language. What’s Apache spark? It’s a fast and general-purpose cluster computing system for large scale data processing. Some important things to know about spark are: . Dataframe: This is data in the form of a table, just like a relational database table. | Dataset: Extension of Dataframes, they provide the functionality of being type-safe and an object-oriented programming interface. | SparkSession vs SparkContext: Spark session is a unified entry point of a spark application from Spark 2.0. It provides a way to interact with various spark’s functionality with a lesser number of constructs. Instead of having a spark context, hive context, SQL context, now all of it is encapsulated in a Spark session | Prior Spark 2.0, Spark Context was the entry point of any spark application and used to access all spark features and needed a sparkConf which had all the cluster configs and parameters to create a Spark Context object | . What’s a spark job? In a spark application, when you invoke an action on RDD, a job is created. It’s the main function that has to be done and submitted to spark. What’s a vault? Hashicorp Vault is a tool for secrets management, encryption as a service, and privileged access management. What’s an S3 bucket? Just a simple distributed file storage system, think of it like the hard disk on your computer . The technology stack Approach . The spark job . authenticates the vault and gets the secrets | reads the updated data from MongoDB | reads the data stored as parquet in S3 bucket | does a left anti join on s3 and mongo data, you now have the data that did not change | merge the data that didn’t change with the new data with the new data | write the data as parquet in S3 bucket | . The authentication was a little better than just sending a saved token: . Grab the EC2 metadata and nonce | Send the metadata and nonce to Vault | Vault checks if the EC2 instance is allowed and the nonce is correct | Obtain secrets like AWS access and secret key, and MongoDB password | Send a nonce to the Vault server and get the token | Get the secrets via the token | . Himanshu(My manager) explained me the approach with a nice example: . Mongo id author book updateTime 1 A1 B1 T1 2 A2 B2 T2 3 A3 B3 T3 4 A4 B4 T4 S3 1 A1 B1 T1 2 A2 B2 T2 3 A3 B3 T2.1 Step 1: get all docs from mongoDb between updateTime1 and updateTime2 where T2 &lt; updateTime1 &lt; updateTime2 fetched records: 3 A3 B3 T3 4 A4 B4 T4 Step 2: read all records from S3. fetched records: 1 A1 B1 T1 2 A2 B2 T2 3 A3 B3 T2.1 Step 3: Left anti join on id between S3 and mongo data so that we have the data that did not change 1 A1 B1 T1 2 A2 B2 T2 Step 4: Merge results of Step 3 with data fetched in Step 1: new data set: 1 A1 B1 T1 2 A2 B2 T2 3 A3 B3 T3 4 A4 B4 T4 This can be put back to S3. You can see how this data is in sync with Mongo data . Challenges (and how I tackled them) . . VPN Issues: The VPN I was using did not support Linux or WSL, so I had to work on windows. Windows is not at all developer-friendly. It was challenging to find a workaround for one-line commands in Linux. However, I found a hack for this. I created an EC2 instance on the Expedia network and shifted all my work to that. Not only I could access all the links, but the scripts ran much faster than on my computer. | . | RAM Issues: I initially was using IntelliJ since it was easier to setup a scala project with it. Intellij was terribly slow and took a lot of RAM. I later switched to VSCode plus Metals. Although it was a little more time consuming to setup, once understood, all RAM issues were solved, additionally using bloop instead of sbt decreased compile time. | Having a local MongoDB server for testing was also quite RAM hungry, MongoDB Atlas helped me a lot here. I deployed a free cluster in the cloud, and my work was done :) | . | Spark and MongoDB Spark has some underlying concepts which were a little nontrivial to understand, at least for a first-timer like me. Above that the documentation of MongoDB-Spark connector was following old spark conventions at one place and new ones somewhere else. The documentation confused me a bit. | . | Understanding Vault EC2 authentication Authentication took most of the time, being a college student, I really didn’t worry about securing my applications in projects. Since we really didn’t put something into production. | I learned about Hashicorp Vault(Store secrets), Terraform(Build instances from code) and Consul(Backend for Vault) | Rather than having a token to authenticate vault and get the secrets, a better method would be to use the ec2 metadata in which the spark job will be running to authenticate vault. | Making EC2 instances with Terraform and Consul | . | Setting up EC2 instances Setting up an instance is easy, but determining the right VPC, subnet, security groups etc. for production really is a security design issue, and one has to decide carefully. | The majority of the AWS services are hacked because of misconfigurations, they aren’t really Amazon’s fault. | . | . Demo . Video: Coming soon. . Next Steps . Clean code, make pull requests. | Try to get the code into production. | . References . Generic . WSL2 , problem with network connection when VPN used (PulseSecure) · Issue #5068 · microsoft/WSL | Stop and remove all docker containers and images | The humble developer | Creating a MongoDB replica set using Docker 🍃 | Slack/Jira integration: How do I post comments to issues, from Slack? | . Task1 : Mongo change streams to S3 via Kafka . What is Apache Kafka®? (A Confluent Lightboard by Tim Berglund) | What is Database Sharding? | edenhill/kafkacat: Generic command line non-JVM Apache Kafka producer and consumer | Mongo Kafka connector docs | Getting Started with the MongoDB Connector for Apache Kafka and MongoDB | Kafka Demo: Confluent Platform Demo (cp-demo) — Confluent Platform | What are change streams? Change streams, a feature introduced in MongoDB 3.6, generate event documents that contain changes to data stored in MongoDB in real-time and provide guarantees of durability, security, and idempotency. You can configure change streams to observe changes at the collection , database , or deployment level. See An Introduction to Change Streams for more information. | . | Using Change Streams to Keep Up with Your Data | MongoDB kafka guide Getting Started with the MongoDB Connector for Apache Kafka and MongoDB | Getting started with the MongoDB Connector for Apache Kafka and MongoDB Atlas | https://stackoverflow.com/questions/57544201/implement-dockerized-kafka-sink-connector-to-mongo/57629665#57629665 | mongodb/mongo-kafka: MongoDB Kafka Connector | . | . Task2: Create a scala job . What is a spark session? | What is Scala? | Apache Spark™ - Unified Analytics Engine for Big Data | Running scala code | Scala Tutorial 2 - Introduction to SBT (Scala Build Tool) | Setup Spark Development Environment – IntelliJ and Scala | Kaizen | Reference code: | nscala-time/nscala-time: A new Scala wrapper for Joda Time based on scala-time | how can I save an isodate field using MongoSpark.save from mongodb spark connector v2.1? | https://spark.apache.org/docs/latest/sql-programming-guide.html#data-types | How to Master Anti Joins and Apply Them to Business Problems | MongoDB and Apache Spark - Getting started tutorial | https://docs.databricks.com/data/data-sources/read-parquet.html#reading-parquet-files-notebook | Spark Read and Write Apache Parquet file — Spark by {Examples} | Left Anti join in Spark? | Spark DataFrame Union and UnionAll — Spark by {Examples} | SQL A left join B, just A? | Install hadoop in 5 steps | Spark s3 | PaaS | Setup scala project with Metals and Bloop (Much better than intellij!) | blog for scala and bloop | Scala and Sbt using the command line | Overwrite a parquet file you just read | https://github.expedia.biz/scala-platform/ha-scala-commons/tree/master/container/config/config-server-aws-vault/src/main/scala/com/homeaway/hascala/container/config/aws | https://stackoverflow.com/questions/41253309/connect-to-spark-running-on-vm | What is Qubole? | What is data lake? | Vault secrets-Wiki | https://www.vaultproject.io/api-docs/auth/aws | Ec2 auth vault | Vault docs AWS | Link local addresses | MicroNugget: What are AWS EC2 Key Pairs? | Vault on AWS: Creating an EC2 Key Pair | terraform variable configuration | terraform setup | .",
            "url": "https://rohanrajpal.github.io/blog/internship/2020/06/17/expedia-internship-experience.html",
            "relUrl": "/internship/2020/06/17/expedia-internship-experience.html",
            "date": " • Jun 17, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "The Virtual Internship: Learning from the Experts [Draft]",
            "content": "Anyone who stops learning is old, whether at twenty or eighty. --Henry Ford . Throughout the 10 weeks in our internship, we had sessions in which we learned more about engineering, testing, coding and leadership! Below, I’ve written what I took out of these sessions; this is a long post, so maybe bring some :coffee:, sit down and enjoy :smile:. If you want to know more about my project. Read here . Leadership Series . Every week, we got a chance to meet the leaders in Expedia and know about their journey. It was quite interesting to know how they solved the problems and inspired others to lead. . . Stephen Lochhead, Global Head of Talent Acquisition 27th May, 2020 I learned the importance of brand building. A good brand helps provides employees with the right direction and motivation | People and diverse opinions can actually turn from a differentiator to a competitive advantage for an organization. | . | . . Greg Schulze, Senior Vice President at Expedia Group 2nd June, 2020 He talked about how the COVID situation is affecting us the previous dark times in travel and how Expedia fought through. | One line he said that I resonate to: Travel gets the best of the Ups and worst of the downs . | . | APAC leaders and Travel Leadership Team (TLT) 9th June, 2020 Dareen Alhiyari, Technology Lead – Retail Product &amp; Technology | Anita Dodia, VP, Engineering - Vrbo | Nitish Khanapure, VP, Product and Technology - Retail Product &amp; Technology | I learned about how the network effect helps scale a business, how Expedia simply isn’t an aggregator of booking places and is much more than that. | . | . Tech Leaders Call . Jonty Neal,Senior Vice President of Expedia Groups Retail Local Markets and Partnerships organization 16th June, 2020 Talked about his journey with Expedia | Where is the company is investing and how are they hedging their risks right now | clickthrough rate | . | . . Todd Reeves,VP Global People Services at Expedia Group 24th June, 2020 If you’re comfortable, change, a really good way to keep growing | Never stop learning, you keep on evolving as you learn more. | . | . Learning Series . Product for Non-PMs 20th May, 2020 Kirsty LaBruniy &amp; Sam Varendorff | I understood the role of a PM(finally! I have been trying to understand that for the last year) | Got to know how PMs took decisions, especially in situations like Covid, where you have to take quick action. | . | . . Agile @ EG 21st May, 2020 Rima Sliat, Technical Product Manager | I got to know about the Agile Methodology. I found it to be a pretty interesting way to work and be as productive as possible. | . | . . Emotional Intelligence 28th May, 2020 Rebecca Shepherd, Principal Program Manager. | Had some interesting discussions about emotional intelligence and how to use it for better decision making in various situations like project reviews, meeting and more. | . | . . Investing in your Mental Wealth 4th June, 2020 Rehana Nanji, Engagement &amp; Learning Manager | Rachel Luff, Inclusion &amp; Learning Programme Manager | Pallavi Tandon, Learning Architect and Behaviour Scientist | I’m glad that this was more of a discussion than a workshop, felt nice to discuss about it and introspect on things we would have just ignored a few years ago. | . | . . Growth Mindset 11th June, 2020 Rebecca Shepherd, Principal Program Manager | I didn’t know about the growth mindset, this really has helped me think about problems with a different angle | Growth mindset is a belief that intelligence can be developed, rather than thinking of it being a gift | . | . . Allyship 18th June, 2020 Paul Berkman, Data Scientist and Full Stack Developer | Pallavi Tandon, Learning Architect and Behaviour Scientist | Focus on biases, did you know that Charlie Chaplin went through depression? if you hadn’t heard about Ted Bundy, would you assume he’s a serial killer just by looking at him? | I understood how biases affect our decisions, how it can aid you act quickly or maybe make wrong assumptions. | . | Personal Branding 25th June, 2020 Pallavi Tandon | We did some exercises to understand our own personal brands. Some of them were: | . If your were an animal, shark or a movie, what would you be ans why? Rohan Rajpal: Animal: Shark, (Sharp and Fast) Object: GoPro, (love adventure) Movie: Fight Club, (Makes you question what is success?) Describe yourself in one word: Rohan Rajpal: Sharing, love to document and share my learnings . I understood about ethos and pathos through these activities | . | . Engineering Series . Code Quality 19th May, 2020 Anuraag Godika, Director of Engineering | Building high-quality software that is safe, secure, and reliable | One has to see the requirements when assessing the code qualities, stuff like magic numbers, nullpointer accesses should be avoided generally | Understood the tradeoffs between making the code more general and overengineering | . | . . Research &amp; Covid-19 Data Work 20th May, 2020 Julie Lauzon, Senior User Researcher | Baraah AlNawaiseh, Lead User Researcher | I understood various tools like eye trackers are used to assess user experience | It was interesting to know how Expedia is using research to reach on the Covid-19 pandemic. | . | . . Platform Architecture 21st May, 2020 James Lewis, Senior Chaos Engineer | I understood chaos testing, how it helps in making robust systems to handle heavy traffic. | If we cant handle heavy traffic, we’re looking at huge losses. | . | . . A/B Testing 21st May, 2020 Guisseppe Gaeta, Technology Lead | John Meakin, Lead Statistician (Head of Experimentation) - ‎Vrbo | Learned how Expedia does different kinds of testing to test potential improvements. | Took up a simple example of changing the landing picture of the Vrbo website and see how it affects the bookings. | . | . . Data Science &amp; Machine Learning 22nd May, 2020 Robert Dickerson, Machine Learning Engineer. | Aditya Mundle, Senior Data Scientist. | I understood how machine learning is applied to real-world applications at Expedia with tools like Spark, Flask, Docker and more. | . | . . Mobile 22nd May, 2020 Rajan Kochhar, Engineering Manager | Gaurav Madaan, Engineering Manager | Introduce Expedia Group mobile application design and environment and learn about the unique challenges of mobile development. (Live &amp; Recorded session) | Met the mobile team of Expedia, got to know that Expedia was the earliest adopters of Kotlin. | Understood the whole pipeline of publishing an app. | . | Security (Hacking and Developing with Security as a focus) 22nd May, 2020 Dale Churchett, Senior Software Architect | Joe Tulowiecki, Staff Application Security Engineer | Discuss hacking and how Expedia meets the threat by building secure networks and applications. | . | . . Test-Driven Development 5th June, 2020 Jason Morefield, Principal SDE | I came to know about a whole new method of development. Test-driven development really helps us build the application step by step while avoiding tough debugging situations. | . | . . The organizing team . Thanks to the organizing team for arranging these sessions! . .",
            "url": "https://rohanrajpal.github.io/blog/internship/2020/06/14/expedia-internship-learning.html",
            "relUrl": "/internship/2020/06/14/expedia-internship-learning.html",
            "date": " • Jun 14, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Selective network routing/Split Tunneling via VPN",
            "content": "Everyone is working from home and for me it is remotely working on the servers in my college. I can only connect to them via a VPN, but that slows down my browsing and video conferencing. . I was trying to figure out a way to use VPN specifically for the server in college and let the other traffic move normally. That’s when I came across this solution. In this blog I’ll explain the solution step by step. . tl,dr : At the end of this blog you’ll be able to select which website or IP addresses you want to route through the VPN. . Requirements: . Ubuntu (Should work on other Linux distros as well) If you are using windows, this method works with Windows Subsystem for Linux. Do ensure that you have WSL2 before proceeding. | . | Openfortivpn Ensure that you can connect to a VPN using it. | . | . Steps for selective routing . 1. VPN configuration file . This file tells our VPN client the configuration of our VPN. . Save the below config file as vpn-config.conf anywhere on your computer . host = vpn.iiitd.edu.in port = 10443 username = &lt;your username&gt; password = &lt;your pass&gt; set-routes = 0 set-dns = 0 pppd-use-peerdns = 0 . set-routes = 0 specifies to not make any routes through the VPN, now we will whitelist the websites to use through the VPN. . 2. Setup the PPP script . What’s PPP?: PPP is Point to Point protocol. Linux uses this protocol to communicate over TCP/IP to your Internet Provider.read more . We are now going to write a script that will whitelist specific domains to pass through the VPN. . Use the following commands to create the script . sudo touch /etc/ppp/ip-up.d/fortivpn sudo chmod a+x /etc/ppp/ip-up.d/fortivpn . What’s pppd? The PPP Daemon (pppd) is a freely available implementation of the Point-to-Point Protocol (PPP) that runs on many Unix systems. read more . What’s ip-up? /etc/ppp/ip-up is a shell script executed by pppd when the link/internet comes up. read more . Edit the above script with your favourite editor, it shall look like: . #!/bin/bash # # Whitelist here all domains that need to go through openfortivpn # Domains and IPs are separated by a space # ips=&#39;192.168.2.217 192.168.29.151&#39; domains=&#39;example.com example.fr&#39; let resolved for domain in $domains; do resolved=`dig +short $domain | tail -n1` ips=&quot;$ips $resolved&quot; done for ip in $ips; do route add $ip dev ppp0 done . Now add the ips and domains you want to access through the VPN. . 3. Run the VPN . The following command should connect you to your VPN now. . sudo openfortivpn -c vpn-config.conf . Below you can see the routes added for the ip addresses. ppp0 is the vpn interface and enp2s0 is the ethernet. . rohan@rohan-laptop ~&gt; route (base) Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface default _gateway 0.0.0.0 UG 100 0 0 enp2s0 one.one.one.one 0.0.0.0 255.255.255.255 UH 0 0 0 ppp0 103.25.231.4 0.0.0.0 255.255.255.255 UH 0 0 0 ppp0 link-local 0.0.0.0 255.255.0.0 U 1000 0 0 enp2s0 192.168.0.0 0.0.0.0 255.255.255.0 U 100 0 0 enp2s0 192.168.2.217 0.0.0.0 255.255.255.255 UH 0 0 0 ppp0 192.168.29.151 0.0.0.0 255.255.255.255 UH 0 0 0 ppp0 . That’s about it! You can now work on your server and enjoy fast internet along :) . Bonus: Automatically start VPN on boot . It’s quite irritating to log into the VPN everytime before starting work. So I created a system service to automatically connect to VPN on boot. Disclaimer: this will not work with WSL2 . Run these commands to setup the service . sudo touch /etc/systemd/system/openfortivpn.service . Open it with your favorite editor and enter this configuration. Thanks to DimitriPapadopoulos for helping me with it. . [Unit] Description = OpenFortiVPN After=network-online.target Documentation=man:openfortivpn(1) [Service] Type=idle ExecStart = /usr/bin/openfortivpn -c &lt;path to your config file&gt; StandardOutput=file:&lt;any-place-where you want to save your logs&gt; Restart=always RestartSec=10 [Install] WantedBy=multi-user.target . To start this service, simply run . sudo systemctl enable openfortivpn sudo systemctl start openfortivpn . To check if it is running . rohan@rohan-laptop ~&gt; sudo systemctl status openfortivpn ● openfortivpn.service - OpenFortiVPN Loaded: loaded (/etc/systemd/system/openfortivpn.service; enabled; vendor preset: enabled) Active: active (running) since Sat 2020-04-25 13:22:26 IST; 3h 43min ago Docs: man:openfortivpn(1) Main PID: 1851 (openfortivpn) Tasks: 6 (limit: 4915) CGroup: /system.slice/openfortivpn.service ├─1851 /usr/bin/openfortivpn -c /home/rohan/Documents/vpn-configs/iiitd.conf └─1852 /usr/sbin/pppd 38400 :1.1.1.1 noipdefault noaccomp noauth default-asyncmap nopcomp Apr 25 13:22:26 rohan-laptop systemd[1]: Started OpenFortiVPN. Apr 25 13:22:26 rohan-laptop pppd[1852]: pppd 2.4.7 started by root, uid 0 Apr 25 13:22:26 rohan-laptop pppd[1852]: Using interface ppp0 Apr 25 13:22:26 rohan-laptop pppd[1852]: Connect: ppp0 &lt;--&gt; /dev/pts/0 Apr 25 13:22:27 rohan-laptop pppd[1852]: local IP address 10.212.134.101 Apr 25 13:22:27 rohan-laptop pppd[1852]: remote IP address 1.1.1.1 . Thanks for reading :) If this did help you, feels free to like, comment and share this blog. . References . openfortivpn | ppp | ppp daemon | thumbnail image | .",
            "url": "https://rohanrajpal.github.io/blog/2020/04/25/Selective-network-routing.html",
            "relUrl": "/2020/04/25/Selective-network-routing.html",
            "date": " • Apr 25, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "DMRC Connect: Improve your daily commute",
            "content": "From July-November 2019, my team and I worked on an application. This project was a part of the course: CSE501-Designing Human-Centered Systems. . DMRC-Connect Media Coverage . We were glad that our project was covered by various news channels. Click on any of the images below to read more: . Our project featured in news Motivation . The metro has been the backbone of transport in Delhi NCR, with daily ridership in excess of 5 million riders. It is famous for providing world-class transit with reach to almost all corners of the city with a reputation of punctuality and quality. At the core of these values lie the vision and mission of DMRC that aim to have the best possible experience for riders from all walks of life and have that with trust and reliability. . DMRC has always tried to take an extra step ahead to connect better with the riders and provide them with the best experience. Social media plays an important role in their endeavour to build this connection, with DMRC being active on all social media platforms and constantly engaging with the riders. . Red Line UpdateDelay in service from Shastri Park towards Dilshad Garden due to a passenger on track at Welcome.Normal service on all other lines. . &mdash; Delhi Metro Rail Corporation (@OfficialDMRC) August 5, 2019 Hi. Metro services shall be resumed as per government directions. We shall update the information on our social media channels. Thank You! . &mdash; Delhi Metro Rail Corporation (@OfficialDMRC) June 12, 2020 However, the question remains, do all Delhi metro users really use Twitter? While DMRC is putting in full efforts to relay this information to riders, is it really reaching the intended audience? Are complaints being solved fast enough for affected riders to benefit from it? Do all complaints be it from Twitter, or from their helplines come with complete information? . Unfortunately, a general trend after rolling out small surveys and talking to users at various metro stations and in and around Delhi, we got to know that this wasn’t the case. Not only was social media outreach restricted to those who followed DMRC on Twitter, but this sort of engagement also was never advertised or encouraged by any official metro source. . Problem Statement . What does DMRC Connect do? . We create an application that facilitates interaction between DMRC officials and DMRC users and bridges the gap created earlier due to less outreach. . What are the features? . Sticking to the theme of the user and official interaction, the features of DMRC Connect are: . Announcements | Complaint registration | Complaint tracking | Quick access helplines | FAQs | The Design Process . Contextual Enquiry . To help understand the problem better and get the user’s perspective on it, we set out to interview general metro users about their usage patterns and general behaviour when it comes to complaints. Our aim was to understand the kind of behaviour users have while interacting with current systems and understand whether the current modes of communication were effective or not. . The target audience was any general rider of the metro who was not a minor, and questions ranged from general ones that helped us understand their general behaviour towards tackling issues they faced, to more specific ones that asked the user about their experiences specific to the metro. . A few of the questions were: . How often do you face “issues” or “problems” in services offered to you? It could be in an app, it could be during transportation like railway/metro/bus, it could be college or conferences? | Have you ever come across official announcements from Delhi metro? Where did you hear about them? | What problems have you faced in the metro till now? Can you describe any one instance? | . Lo-Fidelity Prototypes . Based on the affinity mapping, we came up with the initial paper prototypes of our application and took them to users for testing through Task Analysis. . . Key Findings from v0 and v1: . Users went to “Help Centre” to do most tasks related to getting help from DMRC, very few users went to the actual intended target location. | “Contact Us” seemed to be a way to contact the developers rather than DMRC helplines. | Some people went to the complaint section in v2 to find women’s helpline. | In helplines, instead of direct calls, users wanted phone numbers to pop-up instead. | Hi-Fidelity Prototypes . After 3 iterations of user testing through Task analysis on the lo-fidelity prototypes, we arrived at the first version of the hi-fidelity prototype and conducted testing on it. . High Fidelity prototypes One of the major changes after user testing on the Hi-Fidelity prototype was to change the colour scheme to one with less hue, i.e primarily blue-based instead of red-based, and to shift from side navigation to bottom navigation instead. . Final Color Scheme and Typography . The typography used throughout was Roboto, as it is standardly used by Google and the colour scheme is mentioned below. . . Final Application . The final application, made with help of Android Studio and a Flask backend, was made live on the play store for users to use. The application is available here. However, due to lack of resources, we have closed the backend for now. . The final application DMRC Connect is now Open Source! . We have released the source code on github. Feel free to clone and play around with it! https://github.com/gupta-meghna64/DMRC-Connect . Building Better Interfaces #BBI2019 . Team members . Rohan Rajpal https://github.com/rohanrajpal | Meghna Gupta https://github.com/gupta-meghna64 | Saatvik Jain https://github.com/saatvikj | Tanya Gupta https://github.com/Tanya16107 | Siddharth Yadav https://github.com/sedflix | .",
            "url": "https://rohanrajpal.github.io/blog/mobile/2019/11/15/DMRC-Connect.html",
            "relUrl": "/mobile/2019/11/15/DMRC-Connect.html",
            "date": " • Nov 15, 2019"
        }
        
    
  
    
        ,"post8": {
            "title": "My GSoC experience with VideoLAN: Redesigning the VLC media player",
            "content": "Introduction . This summer, I participated as a Google Summer of Code student under VideoLAN. GSoC is undoubtedly one of the best summer programs out there. From designing interfaces and interactions to writing production-level code, I’ve learned tons of exciting stuff! I was blessed to have great mentors and learned a lot about the open-source community. . Project and Proposal . The VLC media player has an Editor which enables you to customize the player controlbar UI; you can arrange buttons like the play button as per your liking. My main task was to implement this Editor functionality in the new UI for VLC. . You can have a look at my project page on the GSoC website. Feel free to check out my proposal. . Patches . I made an account on VideoLAN’s Gitlab and worked on this repository. I used to push all my commits there to get them reviewed. Then made patches out of them and sent commits to the VLC-developer mailing list. A few other people would then review the patches. Finally, after making the required changes, the patches would get merged. . Here is a list of all my patches. . The Team . A big thanks to the team and my mentors who helped me with my endless doubts! . Abel Tesfaye (GSoC Student) | Sagar Kohli (GSoC Student) | Jean-Baptiste Kempf (VideoLAN president) | Pierre Lamot (Software engineer at Videolabs) | Alexandre Janniaux (Software engineer at Videolabs) | Rohan Rajpal (Me) | . Our communication was mainly via emails and #vlc-gsoc on IRC. . The Project . What work was done? . I had made some contributions towards the player controlbar and further wanted to work on it. Jean then suggested I should work on the Editor. Making the Editor was a big task to do, we divided it into the following parts: . Create a model of all the buttons/widgets on the player ControlBar. | Load the player buttons from the model instead of hardcoding them. | Make a simple drag and drop interface which changes the model by dragging and dropping and updates the config | Make a View from which you can drag and add buttons to the player ControlBar. | Add profiles combobox via which you can load, make and delete configurations for the player. | Populate the miniplayer from the model | Make miniplayer editable and add a tab for it in the editor | I have also worked on a few other things . Add all the missing buttons and widgets. I’ve added the following: Volume Widget | Teletext Widget | Aspect Ratio widget | Record button | Spacer widget | Extended Spacer widget | FullScreen button | Record button | AB Loop button | Snapshot button | Stop button | Media Info button | Frame by frame button | Faster button | Slower button | Open media button | Extended settings button | Step forward button | Step backward button | Quit button | . | Create a topbar for non editable buttons | What’s left to do? . Although I completed every task I was assigned, below-mentioned tasks are best suited as a follow-up for my work done: . If too many widgets come on one side the center buttons don’t remain in the center anymore. This has to be fixed. | The design of the Teletext and a few other widgets isn’t final and work needs to be done. | Demo . Have a look on how the VLC Editor works below: . Highlights and Challenges . Make a generic player controlbar . The first task was to make a model that had all the button and widget data. Then I had to use that model to populate the player ControlBar. Loaders are quite helpful when you have to load a component in QML. . Loader { id: myLoader source: &quot;MyItem.qml&quot; } . After making the buttons model. The next step was to make a QtAbstractListModel to maintain a list of the buttons in current config. This is how the player ControlBar is populated now: . The model would load the config from VLC Core API. | The player controlbar model would then add the respective buttons to the list. | The main controlbar would then use this model to get the list and ids which would then load the button from the buttons model. | . Make sure VLC is accessible via keyboard . One had to make sure that VLC is easily useable via the Keyboard as well. KeyNavigation and Focuscope are the critical things you’ll work with when you are working with Focus. . With a lot of documentation reading, experimenting, and Pierre’s help, I successfully got the KeyNavigation right. . Drag and Drop . We all use drag and drop interfaces quite frequently. Making such interfaces made me understand the design and the logic behind it. . To make an item draggable, you have to set it as a drag target. Similarly, to make it possible to drop a draggable, you need to declare a DropArea. . I also had to add functionalities like move, insert and delete to the model, because Drag and Drop involve all these actions. Have a look at some of the actions below: . The next task was to code the cancel and close buttons. The player controlbar should only be updated when the user presses the close button. To implement this, I used signals. . When you press the close button, the toolbarConfUpdated signal emits, and the playerControlBar is updated. . Signals and Slots are used for communication between objects in Qt. Here’s the signal sent when toolbar is updated: . if( toolbarEditor-&gt;exec() == QDialog::Accepted ) emit toolBarConfUpdated(); . Hinting the user . For the user to easily use the drag and drop interface, we have to provide some hints. When you hover over a draggable item, the cursor changes to an open hand cursor. If you click and hold over it, the cursor changes to a closed hand cursor. The cursor changes to ForbiddenCursor if you take the draggable to a place where it’s not possible to drop. . Profiles . Profiles help someone easily save their preferences. For this, I kept the configs of both the player and the mini-player controlbar and split them using a delimiter. . Different parts of the Editor . One interesting thing about this editor is that it is a mix of QML and Qt/C++. The window, profiles section and action buttons are coded in Qt. The whole drag and drop part is in QML. . Why use Qt when all can be done in QML? Because: . qml takes more ram | qml accessibility is hard to get | qml is harder to debug, and helps doing some big qml files, which we want to avoid at all cost. | . The picture below shows the division. . Things I learned . Git Tricks like fixup and autosquash help a lot in keeping the commits clean. | Rebase and reset is quite helpful when you need to edit or rearrange commits. | How to work with patches, send emails from git directly. | Handling merge conflicts like a pro. | How to split commits? A nice trick is if the commit message has bullet points, it can further be split. | . | Code Avoid writing over-engineered code | How to work on a huge codebase. Things like Memory leaks, ram consumption, learned how to use the VLC Core API. | Design patterns like the D-pointer strategy and Model View Delegate. | Qt/C++, QML and writing production-level code in them. | QtCreator, one of the best IDEs I’ve used. | . | Design Learned design concepts like form follows the flow. | Prototyping, brainstorming on interactions. | Clipping | Thinking design solutions keeping the code in mind. | . | .",
            "url": "https://rohanrajpal.github.io/blog/gsoc/2019/08/12/GSoC-with-VideoLAN.html",
            "relUrl": "/gsoc/2019/08/12/GSoC-with-VideoLAN.html",
            "date": " • Aug 12, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". I’m a third-year student CSE student at IIITD, a Google Summer of Code 2019 student under VideoLAN and an incoming intern at Expedia for Summer 2020. . Here is my Resume. . Notable Coursework . Deep Learning (Ongoing) | Wearable Applications, Research, Devices, Interactions (Ongoing) | Machine Learning | Computer Networks | Designing Human Centered Systems | Analysis and Design of Algorithms | Database Management System | Discrete Maths | Operating Systems | Data Structures and Algorithms | Probability and Statistics | Linear Algebra | . Engineering . Some of my coursework projects include: . Implementing the Go-Back-N protocol in Python | Simulating a computer network using ns-3 and evaluate its performance in C++ | Find diseases in crops using computer vision and deep learning techniques (Pytorch,Fastai). | Find patterns in shallow clouds using satellite imagery using deep learning techniques (Pytorch,Fastai). | Implementing demand paging in Xv6 in C | Implementing priority scheduling in Pintos in C | Implement a custom Linux shell using C | Implementing a Merkle tree in C | Made a replica of the popular Snake Vs Block using Java and JavaFX | . Independent projects: . I wrote a recommender system for movies. Implemented user-user and item-item collaborative filtering in Python. Used Flask, Javascript to build the web app and deployed it to Heroku. | Made a mini-ERP system, an Android app in Java using Firebase for handling the data | . Research . Undergraduate Researcher, Precog. The social computing lab of IIITD. | Working on analzing the social media data of an Indian app. Finding the possibility of an echo chamber within the app. | Using NLP techniques like LDA to do Topic Modelling in text | . Community and Leadership . I like contributing to the community, here’s what I’m trying to do from my side: . Lead and founder, Developer Students Club of IIITD. | Mentor, Student Mentorship Programme: Mentoring 7 freshmen and helping them with their academic, social and personal issues. | . | . Feel free to connect with me on LinkedIn. .",
          "url": "https://rohanrajpal.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  
  

  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://rohanrajpal.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}